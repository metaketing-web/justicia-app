# ğŸ“š Justicia Legal Corpus Collector

Script pour crawler et tÃ©lÃ©charger les textes juridiques officiels de CÃ´te d'Ivoire et de la zone OHADA.

## Sources couvertes

1. **OHADA** (ohada.org) - Actes uniformes
2. **BCEAO/UMOA** (downloads.bceao.int) - RÃ©glementation bancaire et financiÃ¨re
3. **BRVM/CREPMF** (brvm.org) - MarchÃ© financier rÃ©gional
4. **AfricanLII** (agp.africanlii.org) - Textes lÃ©gislatifs CI
5. **JORCI** (alegre.ci) - Journal Officiel de CÃ´te d'Ivoire
6. **UEMOA** (e-docucenter.uemoa.int) - Documents UEMOA

## Installation

```bash
pip install requests beautifulsoup4 pyyaml
```

## Utilisation

```bash
cd scripts
python collector.py --seeds seeds.yml --out ../data/legal_corpus --max-pages 1500 --delay 1.25
```

### Options

- `--seeds` : Fichier YAML contenant les seeds (obligatoire)
- `--out` : Dossier de sortie (dÃ©faut: `data/legal_corpus`)
- `--max-pages` : Nombre maximum de pages HTML par seed (dÃ©faut: 800)
- `--delay` : DÃ©lai entre les requÃªtes en secondes (dÃ©faut: 1.0)

## Structure de sortie

```
data/legal_corpus/
â”œâ”€â”€ ohada_official/
â”‚   â”œâ”€â”€ manifest.jsonl
â”‚   â”œâ”€â”€ acte_uniforme_1.pdf
â”‚   â””â”€â”€ ...
â”œâ”€â”€ bceao_reglementations/
â”‚   â”œâ”€â”€ manifest.jsonl
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## Format du manifest.jsonl

Chaque ligne contient :
```json
{
  "seed": "ohada_official",
  "url": "https://www.ohada.org/...",
  "path": "data/legal_corpus/ohada_official/acte_uniforme_1.pdf",
  "sha256": "abc123...",
  "content_type": "application/pdf",
  "bytes": 123456,
  "downloaded_at": 1703260800
}
```

## IntÃ©gration au RAG

AprÃ¨s le crawl, utilisez le service `legalCitationService.ts` pour :
1. Extraire le texte des PDFs
2. Chunker par article/section
3. Indexer avec mÃ©tadonnÃ©es (source, date, type, rÃ©fÃ©rence)
4. GÃ©nÃ©rer les embeddings

## Respect des conditions d'utilisation

Le script respecte :
- DÃ©lai entre les requÃªtes (rate limiting)
- Domaines autorisÃ©s uniquement
- User-Agent identifiable

âš ï¸ **Important** : VÃ©rifiez les conditions d'utilisation de chaque site avant de lancer le crawler.
